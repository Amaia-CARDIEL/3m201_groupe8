{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  LU3MA201 : Projet / Travail d’étude et de recherche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:AUTHOR: Aya Bouzidi at [Sorbonne Université](http://www.sorbonne-universite.fr/), -->\n",
    "<!-- Author: -->  \n",
    "**Aya Bouzidi** ( L3 de Mathématiques à [Sorbonne Université](http://www.sorbonne-universite.fr/) ).\n",
    "\n",
    "Licence <a href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">CC BY-NC-ND</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Distance tangente comme algorithme de classification de chiffres manuscrits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"ch:method_1\"></div>\n",
    "\n",
    "**Exemples de transformations importantes et leurs dérivées**\n",
    "\n",
    "* **Translation d'axe (Ox)**: $T_X=\\frac{\\partial p}{\\partial x}=p_x$\n",
    "\n",
    "* **Translation d'axe (Oy)**: $T_Y=\\frac{\\partial p}{\\partial y}=p_y$\n",
    "\n",
    "* **Rotation de centre l'origine**: $T_X=y\\frac{\\partial p}{\\partial x}-x\\frac{\\partial p}{\\partial y}=yp_x-xp_y$\n",
    "\n",
    "* **Scaling**: $T_X=x\\frac{\\partial p}{\\partial x}+y\\frac{\\partial p}{\\partial y}=xp_x+yp_y$\n",
    "\n",
    "* **Transformation parallèle hyperbolique**: $T_X=x\\frac{\\partial p}{\\partial x}-y\\frac{\\partial p}{\\partial y}=xp_x-yp_y$\n",
    "\n",
    "* **Transformation diagonale hyperbolique**: $T_X=y\\frac{\\partial p}{\\partial x}+x\\frac{\\partial p}{\\partial y}=yp_x+xp_y$\n",
    "\n",
    "* **Thickening**: $T_X=(\\frac{\\partial p}{\\partial x})^2+(\\frac{\\partial p}{\\partial y})^2=(p_x)^2+(p_y)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"ch:method_1\"></div>\n",
    "\n",
    "Pour différentes transformations importantes du plan, notre algorithme est le suivant: \n",
    "    \n",
    "* **Etape 1:** Pour chaque image de la base d'apprentissage et de la base de tests, calculer sa matrice tangente en fonction de la transformation choisie.\n",
    "\n",
    "\n",
    "* **Etape 2:** Pour chaque image de la base de tests, calculer la distance tangente par rapport à toutes les images de la base d'apprentissage et la classifier selon le chiffre correspondant à l'image qui donne la plus petite distance tangente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"ch:method_1\"></div>\n",
    "\n",
    "**Les instructions suivantes permettent de charger les données de chiffres manuscrits disponibles dans les fichiers base_apprentissage.mat et base_test.mat :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as spi\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat=spi.loadmat(\"base_apprentissage.mat\")\n",
    "data_train=np.transpose(mat['data'])\n",
    "label_train=np.array(mat['label'])[0] #label: chiffre numérisé\n",
    "label_train=label_train.astype(int) #Les labels sont stockés en flottants, on les convertit en entiers\n",
    "\n",
    "mat = spi.loadmat(\"base_test.mat\")\n",
    "data_test = np.transpose(mat['data'])\n",
    "label_test = np.array(mat['label'])[0]\n",
    "label_test =label_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajout overfit\n",
    "data_test_overfit=data_train[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples=[]\n",
    "for i in range(28):\n",
    "    for j in range(28):\n",
    "        tuples+=[(i,j)]\n",
    "        \n",
    "tuples_x=np.array([tuples[i][0] for i in range(784)])\n",
    "tuples_y=np.array([tuples[i][1] for i in range(784)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"ch:method_1\"></div>\n",
    "\n",
    "**On lisse les images:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(v):\n",
    "    P=np.reshape(v,(28,28))\n",
    "    def p(x,y):\n",
    "        S=[P[i,j]*np.e**(-((x-i)**2+(y-j)**2)/(2*0.9**2)) for i,j in tuples]\n",
    "        return sum(S)\n",
    "    return p(tuples_x,tuples_y)\n",
    "\n",
    "def smooth_fcl(v):\n",
    "    P = np.reshape(v, (28,28))\n",
    "    x, y = tuples_x, tuples_y\n",
    "    return sum([P[i,j]*np.e**(-((x-i)**2+(y-j)**2)/(2*0.9**2)) for i,j in tuples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_train=[smooth_fcl(data_train[i]) for i in range(8000)]\n",
    "smooth_test=[smooth_fcl(data_test[i]) for i in range(2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ajout overfit\n",
    "label_test_overfit=label_train[:2000]\n",
    "smooth_test_overfit=smooth_train[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"ch:method_1\"></div>\n",
    "\n",
    "**On calcule les p_x pour les chiffres:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def T_X(v):\n",
    "    P=np.reshape(v,(28,28))\n",
    "    P_=np.gradient(P)[0]\n",
    "    return np.reshape(P_,(1,-1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_x_train=[T_X(smooth_train[i]) for i in range(8000)]\n",
    "p_x_test=[T_X(smooth_test[i]) for i in range(2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ajout overfit\n",
    "p_x_test_overfit=[T_X(smooth_test_overfit[i]) for i in range(2000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"ch:method_1\"></div>\n",
    "\n",
    "**On calcule les p_y pour les chiffres:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def T_Y(v):\n",
    "    P=np.reshape(v,(28,28))\n",
    "    P_=np.gradient(P)[1]\n",
    "    return np.reshape(P_,(1,-1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_y_train=[T_Y(smooth_train[i]) for i in range(8000)]\n",
    "p_y_test=[T_Y(smooth_test[i]) for i in range(2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ajout overfit\n",
    "p_y_test_overfit=[T_Y(smooth_test_overfit[i]) for i in range(2000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Translation d'axe (Ox) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"ch:method_1\"></div>\n",
    "\n",
    "**Fonction de classification pour la X-translation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estim_X(j): #estime l'image data_test[j]\n",
    "    A=[np.hstack((np.reshape(-p_x_train[i],(-1,1)),np.reshape(p_x_test[j],(-1,1)))) for i in range(8000)]\n",
    "    b=[np.reshape(smooth_train[i]-smooth_test[j],(-1,1)) for i in range(8000)]\n",
    "    résidus=[np.linalg.lstsq(A[i], b[i], rcond=None)[1][0] for i in range(8000)] \n",
    "    return label_train[résidus.index(min(résidus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajout overfit\n",
    "\n",
    "def estim_X_overfit(j): #estime l'image data_test_overfit[j]\n",
    "    A=[np.hstack((np.reshape(-p_x_train[i],(-1,1)),np.reshape(p_x_train[j],(-1,1)))) for i in range(8000)]\n",
    "    b=[np.reshape(smooth_train[i]-smooth_train[j],(-1,1)) for i in range(8000)]\n",
    "    x=[np.linalg.lstsq(np.transpose(A[i])@A[i], np.transpose(A[i])@b[i],rcond=None)[0] for i in range(8000)]\n",
    "    résidus=[np.linalg.norm(A[i]@x[i]-b[i]) for i in range(8000)]\n",
    "    return label_train[résidus.index(min(résidus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LISTES A STOCKER\n",
    "%timeit\n",
    "estim_x_trans=[estim_X(i) for i in range(2000)] # pour matrices de confusion\n",
    "estim_x_trans_overfit=[estim_X_overfit(i) for i in range(2000)] # pour overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Translation d'axe (Oy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"ch:method_1\"></div>\n",
    "\n",
    "**Fonction de classification pour la Y-translation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estim_Y(j): #estime l'image data_test[j]\n",
    "    A=[np.hstack((np.reshape(-p_y_train[i],(-1,1)),np.reshape(p_y_test[j],(-1,1)))) for i in range(8000)]\n",
    "    b=[np.reshape(smooth_train[i]-smooth_test[j],(-1,1)) for i in range(8000)]\n",
    "    résidus=[np.linalg.lstsq(A[i], b[i], rcond=None)[1][0] for i in range(8000)] \n",
    "    return label_train[résidus.index(min(résidus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajout overfit\n",
    "\n",
    "def estim_Y_overfit(j): #estime l'image data_test_overfit[j]\n",
    "    A=[np.hstack((np.reshape(-p_y_train[i],(-1,1)),np.reshape(p_y_train[j],(-1,1)))) for i in range(8000)]\n",
    "    b=[np.reshape(smooth_train[i]-smooth_train[j],(-1,1)) for i in range(8000)]\n",
    "    x=[np.linalg.lstsq(np.transpose(A[i])@A[i], np.transpose(A[i])@b[i],rcond=None)[0] for i in range(8000)]\n",
    "    résidus=[np.linalg.norm(A[i]@x[i]-b[i]) for i in range(8000)]\n",
    "    return label_train[résidus.index(min(résidus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LISTES A STOCKER\n",
    "\n",
    "estim_y_trans=[estim_Y(i) for i in range(2000)]\n",
    "estim_y_trans_overfit=[estim_Y_overfit(i) for i in range(2000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_r_train=tuples_y*np.array(p_x_train)-tuples_x*np.array(p_y_train)\n",
    "p_r_test=tuples_y*np.array(p_x_test)-tuples_x*np.array(p_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajout overfit\n",
    "p_r_test_overfit=tuples_y*np.array(p_x_test_overfit)-tuples_x*np.array(p_y_test_overfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"ch:method_1\"></div>\n",
    "\n",
    "**Fonction de classification pour la rotation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estim_R(j): #estime l'image data_test[j]\n",
    "    A=[np.hstack((np.reshape(-p_r_train[i],(-1,1)),np.reshape(p_r_test[j],(-1,1)))) for i in range(8000)]\n",
    "    b=[np.reshape(smooth_train[i]-smooth_test[j],(-1,1)) for i in range(8000)]\n",
    "    résidus=[np.linalg.lstsq(A[i], b[i], rcond=None)[1][0] for i in range(8000)] \n",
    "    return label_train[résidus.index(min(résidus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajout overfit\n",
    "\n",
    "def estim_R_overfit(j): #estime l'image data_test_overfit[j]\n",
    "    A=[np.hstack((np.reshape(-p_r_train[i],(-1,1)),np.reshape(p_r_train[j],(-1,1)))) for i in range(8000)]\n",
    "    b=[np.reshape(smooth_train[i]-smooth_train[j],(-1,1)) for i in range(8000)]\n",
    "    x=[np.linalg.lstsq(np.transpose(A[i])@A[i], np.transpose(A[i])@b[i],rcond=None)[0] for i in range(8000)]\n",
    "    résidus=[np.linalg.norm(A[i]@x[i]-b[i]) for i in range(8000)]\n",
    "    return label_train[résidus.index(min(résidus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LISTES A STOCKER\n",
    "\n",
    "estim_rotation=[estim_R(i) for i in range(2000)]\n",
    "estim_rotation_overfit=[estim_R_overfit(i) for i in range(2000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_s_train=tuples_x*np.array(p_x_train)+tuples_y*np.array(p_y_train)\n",
    "p_s_test=tuples_x*np.array(p_x_test)+tuples_y*np.array(p_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajout overfit\n",
    "p_s_test_overfit=tuples_x*np.array(p_x_test_overfit)+tuples_y*np.array(p_y_test_overfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"ch:method_1\"></div>\n",
    "\n",
    "**Fonction de classification pour le scaling:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estim_S(j): #estime l'image data_test[j]\n",
    "    A=[np.hstack((np.reshape(-p_s_train[i],(-1,1)),np.reshape(p_s_test[j],(-1,1)))) for i in range(8000)]\n",
    "    b=[np.reshape(smooth_train[i]-smooth_test[j],(-1,1)) for i in range(8000)]\n",
    "    résidus=[np.linalg.lstsq(A[i], b[i], rcond=None)[1][0] for i in range(8000)] \n",
    "    return label_train[résidus.index(min(résidus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajout overfit\n",
    "\n",
    "def estim_S_overfit(j): #estime l'image data_test_overfit[j]\n",
    "    A=[np.hstack((np.reshape(-p_s_train[i],(-1,1)),np.reshape(p_s_train[j],(-1,1)))) for i in range(8000)]\n",
    "    b=[np.reshape(smooth_train[i]-smooth_train[j],(-1,1)) for i in range(8000)]\n",
    "    x=[np.linalg.lstsq(np.transpose(A[i])@A[i], np.transpose(A[i])@b[i],rcond=None)[0] for i in range(8000)]\n",
    "    résidus=[np.linalg.norm(A[i]@x[i]-b[i]) for i in range(8000)]\n",
    "    return label_train[résidus.index(min(résidus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LISTES A STOCKER\n",
    "\n",
    "estim_scaling=[estim_S(i) for i in range(2000)]\n",
    "estim_scaling_overfit=[estim_S_overfit(i) for i in range(2000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Transformation parallèle hyperbolique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_TPH_train=tuples_x*np.array(p_x_train)-tuples_y*np.array(p_y_train)\n",
    "p_TPH_test=tuples_x*np.array(p_x_test)-tuples_y*np.array(p_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajout overfit\n",
    "p_TPH_test_overfit=tuples_x*np.array(p_x_test_overfit)-tuples_y*np.array(p_y_test_overfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"ch:method_1\"></div>\n",
    "\n",
    "**Fonction de classification pour la transformation parallèle hyperbolique:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estim_TPH(j): #estime l'image data_test[j]\n",
    "    A=[np.hstack((np.reshape(-p_TPH_train[i],(-1,1)),np.reshape(p_TPH_test[j],(-1,1)))) for i in range(8000)]\n",
    "    b=[np.reshape(smooth_train[i]-smooth_test[j],(-1,1)) for i in range(8000)]\n",
    "    résidus=[np.linalg.lstsq(A[i], b[i], rcond=None)[1][0] for i in range(8000)] \n",
    "    return label_train[résidus.index(min(résidus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajout overfit\n",
    "\n",
    "def estim_TPH_overfit(j): #estime l'image data_test_overfit[j]\n",
    "    A=[np.hstack((np.reshape(-p_TPH_train[i],(-1,1)),np.reshape(p_TPH_train[j],(-1,1)))) for i in range(8000)]\n",
    "    b=[np.reshape(smooth_train[i]-smooth_train[j],(-1,1)) for i in range(8000)]\n",
    "    x=[np.linalg.lstsq(np.transpose(A[i])@A[i], np.transpose(A[i])@b[i],rcond=None)[0] for i in range(8000)]\n",
    "    résidus=[np.linalg.norm(A[i]@x[i]-b[i]) for i in range(8000)]\n",
    "    return label_train[résidus.index(min(résidus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LISTES A STOCKER\n",
    "\n",
    "estim_TPH=[estim_TPH(i) for i in range(2000)]\n",
    "estim_TPH_overfit=[estim_TPH_overfit(i) for i in range(2000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Transformation diagonale hyperbolique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_TDH_train=tuples_y*np.array(p_x_train)+tuples_x*np.array(p_y_train)\n",
    "p_TDH_test=tuples_y*np.array(p_x_test)+tuples_x*np.array(p_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajout overfit\n",
    "p_TDH_test_overfit=tuples_y*np.array(p_x_test_overfit)+tuples_x*np.array(p_y_test_overfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"ch:method_1\"></div>\n",
    "\n",
    "**Fonction de classification pour la transformation diagonale hyperbolique:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estim_TDH(j): #estime l'image data_test[j]\n",
    "    A=[np.hstack((np.reshape(-p_TDH_train[i],(-1,1)),np.reshape(p_TDH_test[j],(-1,1)))) for i in range(8000)]\n",
    "    b=[np.reshape(smooth_train[i]-smooth_test[j],(-1,1)) for i in range(8000)]\n",
    "    résidus=[np.linalg.lstsq(A[i], b[i], rcond=None)[1][0] for i in range(8000)] \n",
    "    return label_train[résidus.index(min(résidus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajout overfit\n",
    "\n",
    "def estim_TDH_overfit(j): #estime l'image data_test_overfit[j]\n",
    "    A=[np.hstack((np.reshape(-p_TDH_train[i],(-1,1)),np.reshape(p_TDH_train[j],(-1,1)))) for i in range(8000)]\n",
    "    b=[np.reshape(smooth_train[i]-smooth_train[j],(-1,1)) for i in range(8000)]\n",
    "    x=[np.linalg.lstsq(np.transpose(A[i])@A[i], np.transpose(A[i])@b[i],rcond=None)[0] for i in range(8000)]\n",
    "    résidus=[np.linalg.norm(A[i]@x[i]-b[i]) for i in range(8000)]\n",
    "    return label_train[résidus.index(min(résidus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LISTES A STOCKER\n",
    "\n",
    "estim_TDH=[estim_TDH(i) for i in range(2000)]\n",
    "estim_TDH_overfit=[estim_TDH_overfit(i) for i in range(2000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Thickening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_T_train=np.array(p_x_train)**2+np.array(p_y_train)**2\n",
    "p_T_test=np.array(p_x_test)**2+np.array(p_y_test)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajout overfit\n",
    "p_T_test_overfit=np.array(p_x_test_overfit)**2+np.array(p_y_test_overfit)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"ch:method_1\"></div>\n",
    "\n",
    "**Fonction de classification pour le thickening:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estim_T(j): #estime l'image data_test[j]\n",
    "    A=[np.hstack((np.reshape(-p_T_train[i],(-1,1)),np.reshape(p_T_test[j],(-1,1)))) for i in range(8000)]\n",
    "    b=[np.reshape(smooth_train[i]-smooth_test[j],(-1,1)) for i in range(8000)]\n",
    "    résidus=[np.linalg.lstsq(A[i], b[i], rcond=None)[1][0] for i in range(8000)] \n",
    "    return label_train[résidus.index(min(résidus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajout overfit\n",
    "\n",
    "def estim_T_overfit(j): #estime l'image data_test_overfit[j]\n",
    "    A=[np.hstack((np.reshape(-p_T_train[i],(-1,1)),np.reshape(p_T_train[j],(-1,1)))) for i in range(8000)]\n",
    "    b=[np.reshape(smooth_train[i]-smooth_train[j],(-1,1)) for i in range(8000)]\n",
    "    x=[np.linalg.lstsq(np.transpose(A[i])@A[i], np.transpose(A[i])@b[i],rcond=None)[0] for i in range(8000)]\n",
    "    résidus=[np.linalg.norm(A[i]@x[i]-b[i]) for i in range(8000)]\n",
    "    return label_train[résidus.index(min(résidus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LISTES A STOCKER\n",
    "\n",
    "estim_thick=[estim_T(i) for i in range(2000)]\n",
    "estim_thick_overfit=[estim_T_overfit(i) for i in range(2000)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
